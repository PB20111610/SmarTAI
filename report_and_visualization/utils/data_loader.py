"""
æ•°æ®æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨æ¨¡å—

åŒ…å«å­¦ç”Ÿæˆç»©ã€ä½œä¸šç»Ÿè®¡ã€é¢˜ç›®åˆ†æç­‰æ•°æ®ç±»ä»¥åŠæ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
from faker import Faker
import random
import json

fake = Faker('zh_CN')

@dataclass
class StudentScore:
    """å­¦ç”Ÿæˆç»©æ•°æ®ç±»"""
    student_id: str
    student_name: str
    total_score: float
    max_score: float
    submit_time: datetime
    questions: List[Dict[str, Any]] = field(default_factory=list)
    need_review: bool = False
    confidence_score: float = 0.85
    
    @property
    def percentage(self) -> float:
        """è®¡ç®—ç™¾åˆ†æ¯”å¾—åˆ†"""
        return (self.total_score / self.max_score) * 100 if self.max_score > 0 else 0
    
    @property
    def grade_level(self) -> str:
        """è·å–æˆç»©ç­‰çº§"""
        percentage = self.percentage
        if percentage >= 90:
            return "ä¼˜ç§€"
        elif percentage >= 80:
            return "è‰¯å¥½"
        elif percentage >= 70:
            return "ä¸­ç­‰"
        elif percentage >= 60:
            return "åŠæ ¼"
        else:
            return "ä¸åŠæ ¼"

@dataclass
class QuestionAnalysis:
    """é¢˜ç›®åˆ†ææ•°æ®ç±»"""
    question_id: str
    question_type: str  # concept, calculation, proof, programming
    topic: str
    difficulty: float  # 0-1
    correct_rate: float
    avg_score: float
    max_score: float
    common_errors: List[str] = field(default_factory=list)
    knowledge_points: List[str] = field(default_factory=list)
    
    @property
    def difficulty_level(self) -> str:
        """è·å–éš¾åº¦ç­‰çº§"""
        if self.difficulty <= 0.3:
            return "ç®€å•"
        elif self.difficulty <= 0.6:
            return "ä¸­ç­‰"
        else:
            return "å›°éš¾"

@dataclass
class AssignmentStats:
    """ä½œä¸šç»Ÿè®¡æ•°æ®ç±»"""
    assignment_id: str
    assignment_name: str
    total_students: int
    submitted_count: int
    avg_score: float
    max_score: float
    min_score: float
    std_score: float
    pass_rate: float
    question_count: int
    create_time: datetime
    
    @property
    def submission_rate(self) -> float:
        """è®¡ç®—æäº¤ç‡"""
        return (self.submitted_count / self.total_students) * 100 if self.total_students > 0 else 0


class DataLoader:
    """æ•°æ®åŠ è½½å™¨ç±» - ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºå¼€å‘æµ‹è¯•"""
    
    def __init__(self, seed: int = 42):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        random.seed(seed)
        np.random.seed(seed)
        Faker.seed(seed)
        
        # é¢„å®šä¹‰çš„çŸ¥è¯†ç‚¹å’Œé¢˜ç›®ç±»å‹
        self.knowledge_points = [
            "çº¿æ€§ä»£æ•°", "å¾®ç§¯åˆ†", "æ¦‚ç‡ç»Ÿè®¡", "æ•°æ®ç»“æ„", "ç®—æ³•è®¾è®¡",
            "é¢å‘å¯¹è±¡ç¼–ç¨‹", "æ•°æ®åº“åŸç†", "è®¡ç®—æœºç½‘ç»œ", "æ“ä½œç³»ç»Ÿ", "è½¯ä»¶å·¥ç¨‹"
        ]
        
        self.question_types = ["concept", "calculation", "proof", "programming"]
        
        self.common_errors = [
            "è®¡ç®—é”™è¯¯", "æ¦‚å¿µç†è§£ä¸å‡†ç¡®", "é€»è¾‘æ¨ç†é”™è¯¯", "è¯­æ³•é”™è¯¯",
            "è¾¹ç•Œæ¡ä»¶å¤„ç†ä¸å½“", "ç®—æ³•æ•ˆç‡ä½ä¸‹", "å†…å­˜æ³„æ¼", "æ ¼å¼é”™è¯¯"
        ]
        
        # æ·»åŠ æ ·ä¾‹é¢˜ç›®å’Œç­”æ¡ˆæ¨¡æ¿
        self.sample_questions = {
            "concept": [
                {
                    "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ•°æ®ç»“æ„ä¸­çš„â€œæ ˆâ€ï¼Œå¹¶è¯´æ˜å…¶ä¸»è¦ç‰¹ç‚¹ã€‚",
                    "standard_answer": "æ ˆæ˜¯ä¸€ç§çº¿æ€§æ•°æ®ç»“æ„ï¼Œéµå¾ªåè¿›å…ˆå‡ºï¼ˆLIFOï¼‰çš„åŸåˆ™ã€‚ä¸»è¦ç‰¹ç‚¹ï¼š1) åªèƒ½åœ¨ä¸€ç«¯è¿›è¡Œæ’å…¥å’Œåˆ é™¤æ“ä½œï¼›2) å…·æœ‰pushå’Œpopä¸¤ä¸ªåŸºæœ¬æ“ä½œï¼›3) æ—¶é—´å¤æ‚åº¦ä¸ºO(1)ã€‚",
                    "points": ["æ­£ç¡®å®šä¹‰æ ˆ", "è¯´æ˜LIFOåŸåˆ™", "åˆ—å‡ºä¸»è¦æ“ä½œ", "åˆ†ææ—¶é—´å¤æ‚åº¦"]
                },
                {
                    "content": "è¯·è¯´æ˜é¢å‘å¯¹è±¡ç¼–ç¨‹ä¸­çš„â€œå°è£…â€æ¦‚å¿µåŠå…¶ä½œç”¨ã€‚",
                    "standard_answer": "å°è£…æ˜¯é¢å‘å¯¹è±¡ç¼–ç¨‹çš„åŸºæœ¬ç‰¹æ€§ä¹‹ä¸€ï¼ŒæŒ‡å°†æ•°æ®å’Œæ“ä½œæ•°æ®çš„æ–¹æ³•ç»„åˆåœ¨ä¸€èµ·ï¼Œå¯¹å¤–éšè—å†…éƒ¨å®ç°ç»†èŠ‚ã€‚ä½œç”¨ï¼š1) æé«˜ä»£ç å®‰å…¨æ€§ï¼›2) é™ä½è€¦åˆåº¦ï¼›3) ä¾¿äºç»´æŠ¤å’Œä¿®æ”¹ã€‚",
                    "points": ["å®šä¹‰å°è£…æ¦‚å¿µ", "è¯´æ˜æ•°æ®éšè—", "åˆ—ä¸¾ä¸»è¦ä½œç”¨"]
                }
            ],
            "calculation": [
                {
                    "content": "è®¡ç®—ä»¥ä¸‹é€’å½’å‡½æ•°çš„æ—¶é—´å¤æ‚åº¦ï¼š\nint fibonacci(int n) {\n    if (n <= 1) return n;\n    return fibonacci(n-1) + fibonacci(n-2);\n}",
                    "standard_answer": "è®¾ T(n) ä¸ºè®¡ç®— fibonacci(n) çš„æ—¶é—´å¤æ‚åº¦ã€‚\né€’æ¨å…³ç³»ï¼šT(n) = T(n-1) + T(n-2) + O(1)\nè§£å¾—ï¼šT(n) = O(2^n)\nå› ä¸ºæ¯ä¸ªè°ƒç”¨éƒ½ä¼šäº§ç”Ÿä¸¤ä¸ªå­é—®é¢˜ï¼Œå½¢æˆäºŒå‰æ ‘ç»“æ„ã€‚",
                    "points": ["å»ºç«‹é€’æ¨å…³ç³»", "æ­£ç¡®æ±‚è§£", "ç»™å‡ºæœ€ç»ˆç»“æœ", "è§£é‡ŠåŸç†"]
                },
                {
                    "content": "å·²çŸ¥ä¸€ä¸ªæ’åºæ•°ç»„ arr = [1, 3, 5, 7, 9, 11, 13]ï¼Œä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç®—æ³•æŸ¥æ‰¾å…ƒç´  7ï¼Œè¯·å†™å‡ºæŸ¥æ‰¾è¿‡ç¨‹ã€‚",
                    "standard_answer": "äºŒåˆ†æŸ¥æ‰¾è¿‡ç¨‹ï¼š\n1. åˆå§‹: left=0, right=6, mid=3, arr[3]=7 âœ“\n2. æ‰¾åˆ°ç›®æ ‡å…ƒç´ ï¼Œè¿”å›ä¸‹æ ‡ 3\næ—¶é—´å¤æ‚åº¦: O(log n)",
                    "points": ["åˆå§‹åŒ–å˜é‡", "æ­£ç¡®è®¡ç®—mid", "åˆ¤æ–­æ¡ä»¶", "ç»™å‡ºç»“æœ"]
                }
            ],
            "proof": [
                {
                    "content": "è¯·è¯æ˜ï¼šå¯¹äºä»»æ„ n ä¸ªèŠ‚ç‚¹çš„äºŒå‰æ ‘ï¼Œå¶å­èŠ‚ç‚¹çš„æ•°é‡ç­‰äºåº¦ä¸º2çš„èŠ‚ç‚¹æ•°é‡åŠ  1ã€‚",
                    "standard_answer": "è¯æ˜ï¼š\nè®¾å¶å­èŠ‚ç‚¹æ•°ä¸º n0ï¼Œåº¦ä¸º1çš„èŠ‚ç‚¹æ•°ä¸º n1ï¼Œåº¦ä¸º2çš„èŠ‚ç‚¹æ•°ä¸º n2ã€‚\næ€»èŠ‚ç‚¹æ•°ï¼š n = n0 + n1 + n2\næ€»è¾¹æ•°ï¼š e = n - 1 = n1 + 2*n2\nç”±äº n = n0 + n1 + n2ï¼Œæ‰€ä»¥ n0 + n1 + n2 - 1 = n1 + 2*n2\nåŒ–ç®€å¾—ï¼š n0 = n2 + 1",
                    "points": ["å®šä¹‰å˜é‡", "å»ºç«‹ç­‰å¼", "æ­£ç¡®æ¨å¯¼", "å¾—å‡ºç»“è®º"]
                }
            ],
            "programming": [
                {
                    "content": "è¯·ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ç°é“¾è¡¨çš„åè½¬æ“ä½œã€‚\nå‡½æ•°ç­¾åï¼šListNode* reverseList(ListNode* head)",
                    "standard_answer": "```cpp\nListNode* reverseList(ListNode* head) {\n    ListNode* prev = nullptr;\n    ListNode* curr = head;\n    \n    while (curr != nullptr) {\n        ListNode* next = curr->next;\n        curr->next = prev;\n        prev = curr;\n        curr = next;\n    }\n    \n    return prev;\n}\n```",
                    "points": ["åˆå§‹åŒ–æŒ‡é’ˆ", "å¾ªç¯åˆ¤æ–­æ¡ä»¶", "æ­£ç¡®æ›´æ–°æŒ‡é’ˆ", "è¿”å›ç»“æœ"]
                }
            ]
        }
    
    def generate_student_scores(self, count: int = 50) -> List[StudentScore]:
        """ç”Ÿæˆå­¦ç”Ÿæˆç»©æ•°æ®"""
        scores = []
        
        for i in range(count):
            # ç”Ÿæˆå­¦ç”ŸåŸºæœ¬ä¿¡æ¯
            student_id = f"2024{str(i+1).zfill(4)}"
            student_name = fake.name()
            
            # ç”Ÿæˆä½œä¸šé¢˜ç›®
            question_count = random.randint(8, 15)
            questions = []
            total_score = 0
            max_total = 0
            
            for j in range(question_count):
                max_score = random.choice([5, 10, 15, 20])
                score = max_score * random.uniform(0.3, 1.0)
                
                # ä½ç½®ä¿¡åº¦æ¦‚ç‡
                confidence = random.uniform(0.6, 0.95)
                
                # ç”Ÿæˆè¯¦ç»†çš„è¯„åˆ†ç»†åˆ™å’Œåˆ¤åˆ†ä¾æ®
                question_type = random.choice(self.question_types)
                grading_rules = self._generate_grading_rules()
                step_analysis = self._generate_step_analysis(score, max_score, question_type)
                model_output = self._generate_model_output(confidence)
                
                # ç”Ÿæˆé¢˜ç›®å†…å®¹å’Œå­¦ç”Ÿç­”æ¡ˆ
                question_content, student_answer, scoring_points = self._generate_question_and_answer(question_type, score, max_score)
                
                question = {
                    "question_id": f"Q{j+1}",
                    "question_type": question_type,
                    "question_title": f"é¢˜ç›®{j+1}: {random.choice(['æ•°æ®ç»“æ„è®¾è®¡', 'ç®—æ³•å®ç°', 'ç¨‹åºåˆ†æ', 'æ¦‚å¿µç†è§£'])}é—®é¢˜",
                    "question_content": question_content["content"],
                    "standard_answer": question_content["standard_answer"],
                    "student_answer": student_answer,
                    "scoring_points": scoring_points,
                    "score": round(score, 1),
                    "max_score": max_score,
                    "confidence": confidence,
                    "feedback": self._generate_feedback(score, max_score),
                    "errors": random.sample(self.common_errors, random.randint(0, 2)) if score < max_score * 0.8 else [],
                    "knowledge_points": random.sample(self.knowledge_points, random.randint(1, 3)),
                    "grading_rules": grading_rules,
                    "step_analysis": step_analysis,
                    "model_output": model_output,
                    "review_notes": self._generate_review_notes(confidence, score, max_score)
                }
                questions.append(question)
                total_score += score
                max_total += max_score
            
            # ç”Ÿæˆæäº¤æ—¶é—´
            submit_time = fake.date_time_between(
                start_date='-7d',
                end_date='now'
            )
            
            # è®¡ç®—æ˜¯å¦éœ€è¦å¤æ ¸
            avg_confidence = np.mean([q["confidence"] for q in questions])
            need_review = avg_confidence < 0.75 or total_score / max_total < 0.6
            
            score_obj = StudentScore(
                student_id=student_id,
                student_name=student_name,
                total_score=round(total_score, 1),
                max_score=max_total,
                submit_time=submit_time,
                questions=questions,
                need_review=need_review,
                confidence_score=avg_confidence
            )
            scores.append(score_obj)
        
        return sorted(scores, key=lambda x: x.total_score, reverse=True)
    
    def generate_question_analysis(self, count: int = 12) -> List[QuestionAnalysis]:
        """ç”Ÿæˆé¢˜ç›®åˆ†ææ•°æ®"""
        analyses = []
        
        for i in range(count):
            question_type = random.choice(self.question_types)
            topic = random.choice(self.knowledge_points)
            difficulty = random.uniform(0.2, 0.9)
            
            # æ ¹æ®éš¾åº¦è®¡ç®—æ­£ç¡®ç‡
            base_correct_rate = 1 - difficulty
            correct_rate = base_correct_rate + random.uniform(-0.2, 0.2)
            correct_rate = max(0.1, min(0.95, correct_rate))
            
            max_score = random.choice([10, 15, 20, 25])
            avg_score = max_score * correct_rate * random.uniform(0.8, 1.2)
            avg_score = max(0, min(max_score, avg_score))
            
            analysis = QuestionAnalysis(
                question_id=f"Q{i+1}",
                question_type=question_type,
                topic=topic,
                difficulty=difficulty,
                correct_rate=correct_rate,
                avg_score=round(avg_score, 1),
                max_score=max_score,
                common_errors=random.sample(self.common_errors, random.randint(1, 4)),
                knowledge_points=random.sample(self.knowledge_points, random.randint(1, 3))
            )
            analyses.append(analysis)
        
        return analyses
    
    def generate_assignment_stats(self) -> AssignmentStats:
        """ç”Ÿæˆä½œä¸šç»Ÿè®¡æ•°æ®"""
        total_students = random.randint(45, 60)
        submitted_count = random.randint(int(total_students * 0.8), total_students)
        
        # ç”Ÿæˆåˆ†æ•°ç»Ÿè®¡
        scores = np.random.normal(75, 15, submitted_count)
        scores = np.clip(scores, 0, 100)
        
        avg_score = np.mean(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        std_score = np.std(scores)
        pass_rate = np.sum(scores >= 60) / len(scores)
        
        return AssignmentStats(
            assignment_id="ASSIGN_001",
            assignment_name="æ•°æ®ç»“æ„ä¸ç®—æ³• - æœŸä¸­ä½œä¸š",
            total_students=total_students,
            submitted_count=submitted_count,
            avg_score=round(avg_score, 1),
            max_score=round(max_score, 1),
            min_score=round(min_score, 1),
            std_score=round(std_score, 1),
            pass_rate=round(pass_rate * 100, 1),
            question_count=12,
            create_time=fake.date_time_between(start_date='-30d', end_date='-7d')
        )
    
    def _generate_feedback(self, score: float, max_score: float) -> str:
        """ç”Ÿæˆè¯„è¯­åé¦ˆ"""
        percentage = (score / max_score) * 100
        
        if percentage >= 90:
            return random.choice([
                "å›ç­”å®Œå…¨æ­£ç¡®ï¼Œæ€è·¯æ¸…æ™°ï¼Œè¡¨è¾¾å‡†ç¡®ï¼",
                "è§£é¢˜æ–¹æ³•æ­£ç¡®ï¼Œæ­¥éª¤æ¸…æ¥šï¼Œå€¼å¾—è¡¨æ‰¬ï¼",
                "å®Œç¾çš„ç­”æ¡ˆï¼Œå±•ç°äº†æ‰å®çš„åŸºç¡€çŸ¥è¯†ï¼"
            ])
        elif percentage >= 80:
            return random.choice([
                "å›ç­”åŸºæœ¬æ­£ç¡®ï¼Œä½†åœ¨ç»†èŠ‚å¤„ç†ä¸Šè¿˜å¯ä»¥æ›´ä»”ç»†ã€‚",
                "æ€è·¯æ­£ç¡®ï¼Œä½†è¡¨è¾¾å¯ä»¥æ›´åŠ å‡†ç¡®ã€‚",
                "è‰¯å¥½çš„è§£ç­”ï¼Œå»ºè®®åœ¨è®¡ç®—ç²¾åº¦ä¸Šå¤šåŠ æ³¨æ„ã€‚"
            ])
        elif percentage >= 60:
            return random.choice([
                "åŸºæœ¬ç†è§£äº†é¢˜æ„ï¼Œä½†åœ¨å…·ä½“å®ç°ä¸Šå­˜åœ¨é—®é¢˜ã€‚",
                "æ€è·¯æœ‰ä¸€å®šæ­£ç¡®æ€§ï¼Œä½†éœ€è¦åŠ å¼ºåŸºç¡€çŸ¥è¯†çš„æŒæ¡ã€‚",
                "éƒ¨åˆ†æ­£ç¡®ï¼Œå»ºè®®å¤ä¹ ç›¸å…³æ¦‚å¿µã€‚"
            ])
        else:
            return random.choice([
                "è§£ç­”å­˜åœ¨è¾ƒå¤§é—®é¢˜ï¼Œå»ºè®®é‡æ–°å­¦ä¹ ç›¸å…³çŸ¥è¯†ç‚¹ã€‚",
                "ç†è§£æœ‰åå·®ï¼Œéœ€è¦åŠ å¼ºåŸºç¡€æ¦‚å¿µçš„å­¦ä¹ ã€‚",
                "ç­”æ¡ˆä¸å¤Ÿå‡†ç¡®ï¼Œå»ºè®®å¯»æ±‚è€å¸ˆæˆ–åŒå­¦çš„å¸®åŠ©ã€‚"
            ])
    
    def _generate_grading_rules(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„åˆ†ç»†åˆ™å’Œåˆ¤åˆ†ä¾æ®"""
        return {
            "scoring_criteria": [
                {
                    "criterion": "ç®—æ³•æ€è·¯æ­£ç¡®æ€§",
                    "weight": 0.4,
                    "points": random.uniform(0.6, 1.0),
                    "description": "è€ƒæŸ¥å­¦ç”Ÿå¯¹ç®—æ³•æ ¸å¿ƒæ€æƒ³çš„ç†è§£"
                },
                {
                    "criterion": "ä»£ç å®ç°è´¨é‡",
                    "weight": 0.3,
                    "points": random.uniform(0.5, 0.9),
                    "description": "ä»£ç çš„å¯è¯»æ€§ã€è§„èŒƒæ€§å’Œæ•ˆç‡"
                },
                {
                    "criterion": "è¾¹ç•Œæƒ…å†µå¤„ç†",
                    "weight": 0.2,
                    "points": random.uniform(0.3, 0.8),
                    "description": "å¯¹å¼‚å¸¸æƒ…å†µå’Œè¾¹ç•Œæ¡ä»¶çš„å¤„ç†"
                },
                {
                    "criterion": "æ³¨é‡Šå’Œæ–‡æ¡£",
                    "weight": 0.1,
                    "points": random.uniform(0.4, 0.9),
                    "description": "ä»£ç æ³¨é‡Šçš„å®Œæ•´æ€§å’Œæ¸…æ™°åº¦"
                }
            ],
            "auto_rules_hit": [
                f"è§„åˆ™R{random.randint(1,10)}: è¯­æ³•æ­£ç¡®æ€§æ£€æŸ¥",
                f"è§„åˆ™R{random.randint(11,20)}: é€»è¾‘ç»“æ„åˆç†æ€§",
                f"è§„åˆ™R{random.randint(21,30)}: å˜é‡å‘½åè§„èŒƒæ€§"
            ][:random.randint(1, 3)]
        }
    
    def _generate_step_analysis(self, score: float, max_score: float, question_type: str = "programming") -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸºäºé¢˜å‹çš„é€æ­¥åˆ†æç»“æœ"""
        # æ ¹æ®é¢˜å‹å®šä¹‰ä¸åŒçš„è¯„åˆ†æ­¥éª¤
        step_templates = {
            "concept": [
                {"title": "æ¦‚å¿µç†è§£å‡†ç¡®æ€§", "weight": 0.4, "threshold": 0.8},
                {"title": "å®šä¹‰è¡¨è¿°å®Œæ•´æ€§", "weight": 0.3, "threshold": 0.7},
                {"title": "ä¸¾ä¾‹ä¸åº”ç”¨", "weight": 0.2, "threshold": 0.6},
                {"title": "è¡¨è¾¾æ¸…æ™°åº¦", "weight": 0.1, "threshold": 0.5}
            ],
            "calculation": [
                {"title": "å…¬å¼é€‰æ‹©ä¸å»ºç«‹", "weight": 0.3, "threshold": 0.8},
                {"title": "è®¡ç®—æ­¥éª¤æ­£ç¡®æ€§", "weight": 0.4, "threshold": 0.7},
                {"title": "æœ€ç»ˆç»“æœå‡†ç¡®æ€§", "weight": 0.2, "threshold": 0.6},
                {"title": "è¿‡ç¨‹è§„èŒƒæ€§", "weight": 0.1, "threshold": 0.5}
            ],
            "proof": [
                {"title": "å‰ææ¡ä»¶è¯†åˆ«", "weight": 0.25, "threshold": 0.8},
                {"title": "é€»è¾‘æ¨ç†è¿‡ç¨‹", "weight": 0.4, "threshold": 0.7},
                {"title": "æ­¥éª¤å®Œæ•´æ€§", "weight": 0.25, "threshold": 0.6},
                {"title": "ç»“è®ºè¡¨è¿°", "weight": 0.1, "threshold": 0.5}
            ],
            "programming": [
                {"title": "é—®é¢˜ç†è§£ä¸åˆ†æ", "weight": 0.2, "threshold": 0.8},
                {"title": "ç®—æ³•è®¾è®¡ä¸é€‰æ‹©", "weight": 0.3, "threshold": 0.7},
                {"title": "ä»£ç å®ç°", "weight": 0.4, "threshold": 0.6},
                {"title": "æµ‹è¯•ä¸éªŒè¯", "weight": 0.1, "threshold": 0.5}
            ]
        }
        
        # è·å–å¯¹åº”é¢˜å‹çš„æ­¥éª¤æ¨¡æ¿
        templates = step_templates.get(question_type, step_templates["programming"])
        
        steps = []
        score_ratio = score / max_score if max_score > 0 else 0
        
        for i, template in enumerate(templates):
            step_score_ratio = min(1.0, max(0.0, score_ratio + random.uniform(-0.2, 0.2)))
            is_correct = step_score_ratio >= template["threshold"]
            
            points_earned = max_score * template["weight"] * step_score_ratio
            max_points = max_score * template["weight"]
            
            # ç”ŸæˆåŸºäºé¢˜å‹å’Œæ­¥éª¤çš„åé¦ˆ
            feedback = self._generate_step_feedback(question_type, template["title"], is_correct, step_score_ratio)
            error_type = self._get_error_type(question_type, template["title"]) if not is_correct else None
            
            step = {
                "step_number": i + 1,
                "step_title": template["title"],
                "is_correct": is_correct,
                "points_earned": round(points_earned, 1),
                "max_points": round(max_points, 1),
                "feedback": feedback,
                "error_type": error_type,
                "highlight": not is_correct
            }
            steps.append(step)
        
        return steps
    
    def _generate_step_feedback(self, question_type: str, step_title: str, is_correct: bool, score_ratio: float) -> str:
        """ç”ŸæˆåŸºäºé¢˜å‹å’Œæ­¥éª¤çš„å…·ä½“åé¦ˆ"""
        feedback_templates = {
            "concept": {
                "æ¦‚å¿µç†è§£å‡†ç¡®æ€§": {
                    True: ["æ¦‚å¿µç†è§£å‡†ç¡®ï¼Œå®šä¹‰æ¸…æ™°", "å¯¹æ ¸å¿ƒæ¦‚å¿µæŠŠæ¡åˆ°ä½", "æ¦‚å¿µæŒæ¡æ‰å®"],
                    False: ["æ¦‚å¿µç†è§£æœ‰åå·®", "æ ¸å¿ƒå®šä¹‰ä¸å‡†ç¡®", "æ¦‚å¿µæ··æ·†"]
                },
                "å®šä¹‰è¡¨è¿°å®Œæ•´æ€§": {
                    True: ["å®šä¹‰è¡¨è¿°å®Œæ•´ï¼Œè¦ç‚¹é½å…¨", "è¡¨è¿°å…¨é¢ï¼Œé€»è¾‘æ¸…æ™°"],
                    False: ["å®šä¹‰ä¸å¤Ÿå®Œæ•´ï¼Œç¼ºå°‘å…³é”®è¦ç‚¹", "è¡¨è¿°è¿‡äºç®€ç•¥"]
                },
                "ä¸¾ä¾‹ä¸åº”ç”¨": {
                    True: ["ä¸¾ä¾‹æ°å½“ï¼Œåº”ç”¨åˆç†", "èƒ½å¤Ÿç»“åˆå®é™…åº”ç”¨"],
                    False: ["ç¼ºå°‘å…·ä½“ä¸¾ä¾‹", "åº”ç”¨åœºæ™¯ç†è§£ä¸è¶³"]
                },
                "è¡¨è¾¾æ¸…æ™°åº¦": {
                    True: ["è¡¨è¾¾æ¸…æ™°ï¼Œé€»è¾‘æ˜ç¡®", "è¯­è¨€å‡†ç¡®ï¼Œæ¡ç†æ¸…æ¥š"],
                    False: ["è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°", "é€»è¾‘ä¸å¤Ÿæ¸…æ¥š"]
                }
            },
            "calculation": {
                "å…¬å¼é€‰æ‹©ä¸å»ºç«‹": {
                    True: ["å…¬å¼é€‰æ‹©æ­£ç¡®ï¼Œå»ºç«‹åˆç†", "æ•°å­¦æ¨¡å‹å‡†ç¡®"],
                    False: ["å…¬å¼é€‰æ‹©é”™è¯¯", "æ•°å­¦å»ºæ¨¡æœ‰é—®é¢˜"]
                },
                "è®¡ç®—æ­¥éª¤æ­£ç¡®æ€§": {
                    True: ["è®¡ç®—æ­¥éª¤æ­£ç¡®ï¼Œè¿‡ç¨‹æ¸…æ™°", "è¿ç®—å‡†ç¡®æ— è¯¯"],
                    False: ["è®¡ç®—è¿‡ç¨‹æœ‰é”™è¯¯", "è¿ç®—æ­¥éª¤ä¸æ­£ç¡®"]
                },
                "æœ€ç»ˆç»“æœå‡†ç¡®æ€§": {
                    True: ["æœ€ç»ˆç»“æœæ­£ç¡®", "ç­”æ¡ˆå‡†ç¡®"],
                    False: ["æœ€ç»ˆç»“æœé”™è¯¯", "ç­”æ¡ˆä¸å‡†ç¡®"]
                },
                "è¿‡ç¨‹è§„èŒƒæ€§": {
                    True: ["è§£é¢˜è¿‡ç¨‹è§„èŒƒï¼Œæ ¼å¼æ­£ç¡®", "æ­¥éª¤å®Œæ•´è§„èŒƒ"],
                    False: ["è¿‡ç¨‹ä¸å¤Ÿè§„èŒƒ", "æ ¼å¼éœ€è¦æ”¹è¿›"]
                }
            },
            "proof": {
                "å‰ææ¡ä»¶è¯†åˆ«": {
                    True: ["å‰ææ¡ä»¶è¯†åˆ«å‡†ç¡®", "å·²çŸ¥æ¡ä»¶ç†è§£æ­£ç¡®"],
                    False: ["å‰ææ¡ä»¶ç†è§£æœ‰è¯¯", "å·²çŸ¥æ¡ä»¶è¯†åˆ«ä¸å‡†ç¡®"]
                },
                "é€»è¾‘æ¨ç†è¿‡ç¨‹": {
                    True: ["é€»è¾‘æ¨ç†ä¸¥å¯†ï¼Œæ­¥éª¤æ¸…æ™°", "æ¨ç†è¿‡ç¨‹åˆç†"],
                    False: ["é€»è¾‘æ¨ç†æœ‰æ¼æ´", "æ¨ç†æ­¥éª¤ä¸ä¸¥å¯†"]
                },
                "æ­¥éª¤å®Œæ•´æ€§": {
                    True: ["è¯æ˜æ­¥éª¤å®Œæ•´", "æ¨å¯¼è¿‡ç¨‹å®Œæ•´"],
                    False: ["è¯æ˜æ­¥éª¤ä¸å®Œæ•´", "ç¼ºå°‘å…³é”®æ¨å¯¼æ­¥éª¤"]
                },
                "ç»“è®ºè¡¨è¿°": {
                    True: ["ç»“è®ºè¡¨è¿°æ¸…æ™°å‡†ç¡®", "æœ€ç»ˆç»“è®ºæ­£ç¡®"],
                    False: ["ç»“è®ºè¡¨è¿°ä¸æ¸…æ¥š", "ç»“è®ºä¸å‡†ç¡®"]
                }
            },
            "programming": {
                "é—®é¢˜ç†è§£ä¸åˆ†æ": {
                    True: ["æ­£ç¡®ç†è§£äº†é—®é¢˜è¦æ±‚ï¼Œåˆ†ææ¸…æ™°", "é—®é¢˜åˆ†æåˆ°ä½"],
                    False: ["é—®é¢˜ç†è§£æœ‰åå·®", "éœ€æ±‚åˆ†æä¸å‡†ç¡®"]
                },
                "ç®—æ³•è®¾è®¡ä¸é€‰æ‹©": {
                    True: ["ç®—æ³•é€‰æ‹©åˆç†", "ç®—æ³•è®¾è®¡æ­£ç¡®"],
                    False: ["ç®—æ³•é€‰æ‹©ä¸å¤Ÿä¼˜åŒ–", "ç®—æ³•è®¾è®¡æœ‰é—®é¢˜"]
                },
                "ä»£ç å®ç°": {
                    True: ["å®ç°æ­£ç¡®", "ä»£ç è´¨é‡è‰¯å¥½"],
                    False: ["å®ç°æœ‰è¾ƒå¤§é—®é¢˜", "ä»£ç å­˜åœ¨é”™è¯¯"]
                },
                "æµ‹è¯•ä¸éªŒè¯": {
                    True: ["æµ‹è¯•å……åˆ†", "éªŒè¯å®Œæ•´"],
                    False: ["ç¼ºå°‘å¿…è¦æµ‹è¯•", "éªŒè¯ä¸å……åˆ†"]
                }
            }
        }
        
        templates = feedback_templates.get(question_type, feedback_templates["programming"])
        step_templates = templates.get(step_title, {True: ["è¡¨ç°è‰¯å¥½"], False: ["éœ€è¦æ”¹è¿›"]})
        feedback_list = step_templates.get(is_correct, ["è¯„ä»·ä¸­"])
        
        return random.choice(feedback_list)
    
    def _get_error_type(self, question_type: str, step_title: str) -> str:
        """æ ¹æ®é¢˜å‹å’Œæ­¥éª¤è·å–é”™è¯¯ç±»å‹"""
        error_types = {
            "concept": {
                "æ¦‚å¿µç†è§£å‡†ç¡®æ€§": "æ¦‚å¿µç†è§£é”™è¯¯",
                "å®šä¹‰è¡¨è¿°å®Œæ•´æ€§": "å®šä¹‰ä¸å®Œæ•´",
                "ä¸¾ä¾‹ä¸åº”ç”¨": "åº”ç”¨ç†è§£ä¸è¶³",
                "è¡¨è¾¾æ¸…æ™°åº¦": "è¡¨è¾¾ä¸æ¸…"
            },
            "calculation": {
                "å…¬å¼é€‰æ‹©ä¸å»ºç«‹": "å…¬å¼é”™è¯¯",
                "è®¡ç®—æ­¥éª¤æ­£ç¡®æ€§": "è®¡ç®—é”™è¯¯",
                "æœ€ç»ˆç»“æœå‡†ç¡®æ€§": "ç»“æœé”™è¯¯",
                "è¿‡ç¨‹è§„èŒƒæ€§": "æ ¼å¼é—®é¢˜"
            },
            "proof": {
                "å‰ææ¡ä»¶è¯†åˆ«": "å‰æç†è§£é”™è¯¯",
                "é€»è¾‘æ¨ç†è¿‡ç¨‹": "é€»è¾‘é”™è¯¯",
                "æ­¥éª¤å®Œæ•´æ€§": "æ­¥éª¤ä¸å®Œæ•´",
                "ç»“è®ºè¡¨è¿°": "ç»“è®ºé”™è¯¯"
            },
            "programming": {
                "é—®é¢˜ç†è§£ä¸åˆ†æ": "éœ€æ±‚ç†è§£é”™è¯¯",
                "ç®—æ³•è®¾è®¡ä¸é€‰æ‹©": "ç®—æ³•æ•ˆç‡é—®é¢˜",
                "ä»£ç å®ç°": "é€»è¾‘é”™è¯¯",
                "æµ‹è¯•ä¸éªŒè¯": "æµ‹è¯•ä¸å……åˆ†"
            }
        }
        
        types = error_types.get(question_type, error_types["programming"])
        return types.get(step_title, "å…¶ä»–é”™è¯¯")
    
    def _generate_model_output(self, confidence: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹è¾“å‡ºå’Œè¿è¡Œæ—¥å¿—"""
        return {
            "model_name": random.choice(["GPT-4-Turbo", "Claude-3-Opus", "Gemini-Pro", "Custom-Grader-v2.1"]),
            "processing_time": f"{random.uniform(0.5, 3.2):.2f}s",
            "confidence_score": confidence,
            "reasoning_tokens": random.randint(150, 800),
            "output_summary": "æ¨¡å‹è¯†åˆ«å‡ºä»£ç ç»“æ„æ¸…æ™°ï¼Œä½†åœ¨ç®—æ³•ä¼˜åŒ–æ–¹é¢å­˜åœ¨æ”¹è¿›ç©ºé—´" if confidence < 0.8 else "æ¨¡å‹è¯†åˆ«å‡ºä¼˜ç§€çš„è§£å†³æ–¹æ¡ˆï¼Œç¬¦åˆæœ€ä½³å®è·µ",
            "log_entries": [
                f"[INFO] å¼€å§‹åˆ†æä»£ç ç»“æ„... (timestamp: {random.randint(1000, 9999)}ms)",
                f"[DEBUG] æ£€æµ‹åˆ°å‡½æ•°å®šä¹‰: {random.randint(3, 8)}ä¸ª",
                f"[INFO] è¯­æ³•åˆ†æå®Œæˆï¼Œç¼–è¯‘é€šè¿‡",
                f"[WARN] å‘ç°æ½œåœ¨æ€§èƒ½é—®é¢˜: å¾ªç¯å¤æ‚åº¦" if confidence < 0.75 else f"[INFO] æ€§èƒ½åˆ†æé€šè¿‡",
                f"[INFO] ç½®ä¿¡åº¦è¯„ä¼°å®Œæˆ: {confidence:.2%}"
            ]
        }
    
    def _generate_review_notes(self, confidence: float, score: float, max_score: float) -> Dict[str, Any]:
        """ç”Ÿæˆå¤æ ¸å»ºè®®å’Œæ³¨æ„äº‹é¡¹"""
        needs_review = confidence < 0.75 or (score / max_score) < 0.6
        
        return {
            "needs_review": needs_review,
            "review_priority": "High" if confidence < 0.6 else "Medium" if confidence < 0.75 else "Low",
            "review_reasons": [
                "ç½®ä¿¡åº¦ä½äºé˜ˆå€¼" if confidence < 0.75 else None,
                "å¾—åˆ†ä½äºæ ‡å‡†" if (score / max_score) < 0.6 else None,
                "æ¨¡å‹è¾“å‡ºä¸ä¸€è‡´" if confidence < 0.65 else None
            ],
            "suggested_actions": [
                "äººå·¥å¤æ ¸ä»£ç é€»è¾‘" if needs_review else None,
                "æ£€æŸ¥è¯„åˆ†æ ‡å‡†ä¸€è‡´æ€§" if confidence < 0.7 else None,
                "ä¸å­¦ç”Ÿæ²Ÿé€šç¡®è®¤" if (score / max_score) < 0.5 else None
            ],
            "estimated_review_time": f"{random.randint(5, 20)}åˆ†é’Ÿ" if needs_review else "0åˆ†é’Ÿ"
        }
    
    def _generate_question_and_answer(self, question_type: str, score: float, max_score: float) -> tuple:
        """ç”Ÿæˆé¢˜ç›®å†…å®¹å’Œå­¦ç”Ÿç­”æ¡ˆ"""
        # ä»æ ·ä¾‹é¢˜ç›®ä¸­éšæœºé€‰æ‹©
        question_template = random.choice(self.sample_questions[question_type])
        
        # ç”Ÿæˆå­¦ç”Ÿç­”æ¡ˆï¼ˆæ ¹æ®å¾—åˆ†æƒ…å†µç”Ÿæˆä¸åŒè´¨é‡çš„ç­”æ¡ˆï¼‰
        percentage = (score / max_score) * 100
        student_answer = self._generate_student_answer(question_type, question_template, percentage)
        
        # ç”Ÿæˆè¯„åˆ†ç‚¹åˆ†æ
        scoring_points = self._generate_scoring_points_analysis(question_template["points"], percentage)
        
        return question_template, student_answer, scoring_points
    
    def _generate_student_answer(self, question_type: str, question_template: dict, percentage: float) -> str:
        """æ ¹æ®å¾—åˆ†æƒ…å†µç”Ÿæˆå­¦ç”Ÿç­”æ¡ˆ"""
        if percentage >= 90:  # ä¼˜ç§€ç­”æ¡ˆ
            return self._generate_excellent_answer(question_type, question_template)
        elif percentage >= 75:  # è‰¯å¥½ç­”æ¡ˆ
            return self._generate_good_answer(question_type, question_template)
        elif percentage >= 60:  # ä¸­ç­‰ç­”æ¡ˆ
            return self._generate_average_answer(question_type, question_template)
        else:  # è¾ƒå·®ç­”æ¡ˆ
            return self._generate_poor_answer(question_type, question_template)
    
    def _generate_excellent_answer(self, question_type: str, template: dict) -> str:
        """ç”Ÿæˆä¼˜ç§€ç­”æ¡ˆ"""
        if question_type == "concept":
            return template["standard_answer"] + "é™„åŠ è¯´æ˜ï¼šè¿˜å¯ä»¥ç»“åˆå®é™…åº”ç”¨åœºæ™¯æ¥ç†è§£ã€‚"
        elif question_type == "calculation":
            return template["standard_answer"] + "\n\néªŒè¯ï¼šå¯ä»¥é€šè¿‡ä»£å…¥å…·ä½“æ•°å€¼è¿›è¡Œæ£€éªŒã€‚"
        elif question_type == "proof":
            return template["standard_answer"] + "\n\næ³¨ï¼šè¿™ä¸ªç»“è®ºåœ¨æ ‘ç»“æ„åˆ†æä¸­éå¸¸é‡è¦ã€‚"
        else:  # programming
            return template["standard_answer"] + "\n\n// æ—¶é—´å¤æ‚åº¦: O(n), ç©ºé—´å¤æ‚åº¦: O(1)"
    
    def _generate_good_answer(self, question_type: str, template: dict) -> str:
        """ç”Ÿæˆè‰¯å¥½ç­”æ¡ˆï¼ˆæœ‰å°ç¼ºé™·ï¼‰"""
        if question_type == "concept":
            # ç¼ºå°‘ä¸€äº›ç»†èŠ‚
            answer = template["standard_answer"]
            return answer.replace("æ—¶é—´å¤æ‚åº¦ä¸ºO(1)", "æ“ä½œå¾ˆå¿«")
        elif question_type == "calculation":
            # ç­”æ¡ˆæ­£ç¡®ä½†ç¼ºå°‘è§£é‡Š
            lines = template["standard_answer"].split('\n')
            return '\n'.join(lines[:-1])  # å»æ‰æœ€åä¸€è¡Œè§£é‡Š
        elif question_type == "proof":
            # è¯æ˜æ­¥éª¤æ­£ç¡®ä½†ç•¥æ˜¾ç²—ç³™
            return template["standard_answer"].replace("åŒ–ç®€å¾—ï¼š", "æ‰€ä»¥ï¼š")
        else:  # programming
            # ä»£ç æ­£ç¡®ä½†ç¼ºå°‘æ³¨é‡Š
            return template["standard_answer"].replace("    ", "  ")  # ç¼©è¿›ä¸è§„èŒƒ
    
    def _generate_average_answer(self, question_type: str, template: dict) -> str:
        """ç”Ÿæˆä¸­ç­‰ç­”æ¡ˆï¼ˆæœ‰æ˜æ˜¾é”™è¯¯ï¼‰"""
        if question_type == "concept":
            return "æ ˆæ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œéµå¾ªFIFOåŸåˆ™ã€‚ä¸»è¦ç‰¹ç‚¹æ˜¯å¯ä»¥åœ¨ä¸¤ç«¯è¿›è¡Œæ“ä½œã€‚"  # LIFOè¯´æˆFIFO
        elif question_type == "calculation":
            return "è®¾ T(n) = T(n-1) + T(n-2)\næ‰€ä»¥ T(n) = O(n^2)"  # ç»“æœé”™è¯¯
        elif question_type == "proof":
            return "è¯æ˜ï¼š\nè®¾å¶å­èŠ‚ç‚¹æ•°ä¸º n0ï¼Œåº¦ä¸º2çš„èŠ‚ç‚¹æ•°ä¸º n2ã€‚\nå› ä¸º n0 = n2ï¼Œæ‰€ä»¥ç»“è®ºæˆç«‹ã€‚"  # æ¨ç†è¿‡ç¨‹ä¸å®Œæ•´
        else:  # programming
            return "```cpp\nListNode* reverseList(ListNode* head) {\n    ListNode* prev = nullptr;\n    while (head != nullptr) {\n        prev = head;\n        head = head->next;\n    }\n    return prev;\n}\n```"  # é€»è¾‘é”™è¯¯
    
    def _generate_poor_answer(self, question_type: str, template: dict) -> str:
        """ç”Ÿæˆè¾ƒå·®ç­”æ¡ˆï¼ˆå­˜åœ¨é‡å¤§é”™è¯¯ï¼‰"""
        if question_type == "concept":
            return "æ ˆæ˜¯ä¸€ç§æ•°ç»„ï¼Œå¯ä»¥å­˜å‚¨æ•°æ®ã€‚"  # æ¦‚å¿µé”™è¯¯
        elif question_type == "calculation":
            return "è¿™ä¸ªç®—æ³•å¾ˆå¿«ï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯ O(1)ã€‚"  # å®Œå…¨é”™è¯¯
        elif question_type == "proof":
            return "è¿™ä¸ªç»“è®ºæ˜¯å¯¹çš„ï¼Œå› ä¸ºæˆ‘ä»¬è€å¸ˆè®²è¿‡ã€‚"  # æ— è¯æ˜è¿‡ç¨‹
        else:  # programming
            return "```cpp\nListNode* reverseList(ListNode* head) {\n    return head;\n}\n```"  # æ²¡æœ‰å®ç°
    
    def _generate_scoring_points_analysis(self, points: List[str], percentage: float) -> List[dict]:
        """ç”Ÿæˆè¯„åˆ†ç‚¹åˆ†æ"""
        analysis = []
        points_per_item = 100 / len(points)
        
        for i, point in enumerate(points):
            # æ ¹æ®æ€»ä½“å¾—åˆ†ç‡ç”Ÿæˆå„ä¸ªå¾—åˆ†ç‚¹çš„æƒ…å†µ
            if percentage >= 90:
                earned = random.uniform(0.9, 1.0) * points_per_item
                status = "excellent"
            elif percentage >= 75:
                earned = random.uniform(0.7, 0.9) * points_per_item
                status = "good" if i < len(points) - 1 else "average"  # æœ€åä¸€ä¸ªç‚¹å¯èƒ½æœ‰é—®é¢˜
            elif percentage >= 60:
                earned = random.uniform(0.4, 0.8) * points_per_item
                status = "average" if i < len(points) // 2 else "poor"  # ååŠéƒ¨åˆ†å¾—åˆ†ä½
            else:
                earned = random.uniform(0.0, 0.5) * points_per_item
                status = "poor"
            
            analysis.append({
                "point": point,
                "earned_percentage": min(100, max(0, earned)),
                "status": status,
                "feedback": self._get_point_feedback(point, status)
            })
        
        return analysis
    
    def _get_point_feedback(self, point: str, status: str) -> str:
        """æ ¹æ®å¾—åˆ†ç‚¹çŠ¶æ€ç”Ÿæˆåé¦ˆ"""
        feedback_map = {
            "excellent": "âœ… å®Œå…¨æ­£ç¡®",
            "good": "ğŸŸ¡ åŸºæœ¬æ­£ç¡®ï¼Œæœ‰å°ç¼ºé™·",
            "average": "ğŸŸ  éƒ¨åˆ†æ­£ç¡®ï¼Œå­˜åœ¨é”™è¯¯",
            "poor": "âŒ ä¸æ­£ç¡®æˆ–ç¼ºå¤±"
        }
        return feedback_map.get(status, "âšª æ— è¯„ä»·")
    
    def get_sample_data(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„ç¤ºä¾‹æ•°æ®é›†"""
        return {
            "student_scores": self.generate_student_scores(50),
            "question_analysis": self.generate_question_analysis(12),
            "assignment_stats": self.generate_assignment_stats()
        }


# å…¨å±€æ•°æ®åŠ è½½å™¨å®ä¾‹
data_loader = DataLoader()

def load_sample_data() -> Dict[str, Any]:
    """åŠ è½½ç¤ºä¾‹æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    return data_loader.get_sample_data()

def get_student_scores(count: int = 50) -> List[StudentScore]:
    """è·å–å­¦ç”Ÿæˆç»©æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    return data_loader.generate_student_scores(count)

def get_question_analysis(count: int = 12) -> List[QuestionAnalysis]:
    """è·å–é¢˜ç›®åˆ†ææ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    return data_loader.generate_question_analysis(count)

def get_assignment_stats() -> AssignmentStats:
    """è·å–ä½œä¸šç»Ÿè®¡æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    return data_loader.generate_assignment_stats()