import streamlit as st
from streamlit_scroll_to_top import scroll_to_here
import requests
import pandas as pd
from utils import *
import json
import os
import re
import datetime

# --- é¡µé¢åŸºç¡€è®¾ç½® ---
st.set_page_config(
    page_title="AIæ‰¹æ”¹ç»“æœ - æ™ºèƒ½ä½œä¸šæ ¸æŸ¥ç³»ç»Ÿ",
    layout="wide",
    page_icon="ğŸ“Š"
)

initialize_session_state()

# åœ¨æ¯ä¸ªé¡µé¢çš„é¡¶éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°
load_custom_css()

def render_header():
    """æ¸²æŸ“é¡µé¢å¤´éƒ¨"""
    col1, col2, col3, col4, col5, col6, col7, col8 = st.columns(8)
    col = st.columns(1)[0]

    with col1:
        st.page_link("main.py", label="è¿”å›é¦–é¡µ", icon="ğŸ ")
    
    with col2:
        st.page_link("pages/history.py", label="å†å²è®°å½•", icon="ğŸ•’")

    with col3:
        st.page_link("pages/problems.py", label="ä½œä¸šé¢˜ç›®", icon="ğŸ“–")

    with col4:
        st.page_link("pages/stu_preview.py", label="å­¦ç”Ÿä½œä¸š", icon="ğŸ“")
    
    with col5:
        st.page_link("pages/grade_results.py", label="æ‰¹æ”¹ç»“æœ", icon="ğŸ“Š")

    with col6:
        st.page_link("pages/score_report.py", label="è¯„åˆ†æŠ¥å‘Š", icon="ğŸ’¯")

    with col7:
        st.page_link("pages/visualization.py", label="æˆç»©åˆ†æ", icon="ğŸ“ˆ")

    with col8:
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®", use_container_width=False):
            st.rerun()
    
    with col:
        st.markdown("<h1 style='text-align: center; color: #000000;'>ğŸ“Š AIæ‰¹æ”¹ç»“æœæ€»è§ˆ</h1>", 
                   unsafe_allow_html=True)
 
render_header()

# --- å®‰å…¨æ£€æŸ¥ (å·²ä¿®å¤) ---

# 1. ç¡®ä¿ st.session_state.jobs æ˜¯ä¸€ä¸ªå­—å…¸
if "jobs" not in st.session_state:
    st.session_state.jobs = {}

# 2. å¦‚æœæœ‰ä»æäº¤é¡µé¢ä¼ æ¥çš„æ–°ä»»åŠ¡IDï¼Œå°±å°†å…¶â€œæ·»åŠ â€åˆ° jobs å­—å…¸ä¸­ï¼Œè€Œä¸æ˜¯è¦†ç›–
if "current_job_id" in st.session_state:
    new_job_id = st.session_state.current_job_id
    if new_job_id not in st.session_state.jobs:
        # ä½¿ç”¨å­—å…¸çš„ update æ–¹æ³•æˆ–ç›´æ¥èµ‹å€¼æ¥â€œæ·»åŠ â€æ–°ä»»åŠ¡
        st.session_state.jobs[new_job_id] = {"name": f"æœ€æ–°æ‰¹æ”¹ä»»åŠ¡ - {new_job_id}", "submitted_at": {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}}
    
    # å°†å½“å‰é€‰ä¸­çš„ä»»åŠ¡è®¾ç½®ä¸ºè¿™ä¸ªæ–°ä»»åŠ¡
    st.session_state.selected_job_id = new_job_id
    
    # æ¸…ç†æ‰ä¸´æ—¶çš„ current_job_id
    del st.session_state.current_job_id

# 3. å¦‚æœæ²¡æœ‰ä»»ä½•ä»»åŠ¡è®°å½•ï¼Œåˆ™æç¤ºå¹¶åœæ­¢
if not st.session_state.jobs:
    st.warning("å½“å‰æ²¡æœ‰æ‰¹æ”¹ä»»åŠ¡è®°å½•ã€‚")
    st.stop()

# 4. è·å–å½“å‰åº”è¯¥é€‰æ‹©çš„ä»»åŠ¡ID
selected_job_id = st.session_state.get("selected_job_id")

# ... åç»­ä»£ç ä¸å˜ ...

# Filter out mock jobs
filtered_jobs = {}
if "jobs" in st.session_state:
    for job_id, job_info in st.session_state.jobs.items():
        # Skip mock jobs
        if not job_id.startswith("MOCK_JOB_") and not job_info.get("is_mock", False):
            filtered_jobs[job_id] = job_info
    st.session_state.jobs = filtered_jobs

# Get job IDs after filtering
job_ids = list(st.session_state.jobs.keys()) if "jobs" in st.session_state else []

# --- é¡µé¢å†…å®¹ ---
# st.title("ğŸ“Š AIæ‰¹æ”¹ç»“æœ")

# # Add debug button
# if st.button("è°ƒè¯•ï¼šæ£€æŸ¥æ‰€æœ‰ä»»åŠ¡"):
#     from frontend_utils.data_loader import check_all_jobs
#     all_jobs = check_all_jobs()
#     st.write("æ‰€æœ‰ä»»åŠ¡çŠ¶æ€:", all_jobs)

# æ˜ å°„é¢˜ç›®ç±»å‹ï¼šä»å†…éƒ¨ç±»å‹åˆ°ä¸­æ–‡æ˜¾ç¤ºåç§°
type_display_mapping = {
    "concept": "æ¦‚å¿µé¢˜",
    "calculation": "è®¡ç®—é¢˜", 
    "proof": "è¯æ˜é¢˜",
    "programming": "ç¼–ç¨‹é¢˜"
}

def natural_sort_key(s):
    """
    å®ç°è‡ªç„¶æ’åºçš„è¾…åŠ©å‡½æ•°ã€‚
    ä¾‹å¦‚: "q2" ä¼šæ’åœ¨ "q10" ä¹‹å‰ã€‚
    """
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    s = str(s)
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]

# Get job IDs
job_ids = list(st.session_state.jobs.keys())
if not job_ids:
    st.info("æ²¡æœ‰æ‰¾åˆ°æ‰¹æ”¹ä»»åŠ¡ã€‚")
else:
    # If we came from the history page with a selected job, use that
    # Otherwise, let the user select a job
    if selected_job_id and selected_job_id in st.session_state.jobs:
        selected_job = selected_job_id
        # Don't clear the selected job ID from session state, keep it for other pages
        # del st.session_state.selected_job_id
    elif "current_job_id" in st.session_state and st.session_state.current_job_id in st.session_state.jobs:
        selected_job = st.session_state.current_job_id
        # Clean up the temporary job ID
        del st.session_state.current_job_id
    else:
        selected_job = st.selectbox(
            "é€‰æ‹©ä¸€ä¸ªæ‰¹æ”¹ä»»åŠ¡",
            job_ids,
            format_func=lambda x: st.session_state.jobs[x].get("name", x)
        )
    
    if selected_job:
        # è·å–ä»»åŠ¡è¯¦æƒ…
        st.session_state.selected_job_id = selected_job
        task_info = st.session_state.jobs[selected_job]
        st.subheader(f"ä»»åŠ¡: {task_info.get('name', 'æœªçŸ¥ä»»åŠ¡')}")
        st.write(f"æäº¤æ—¶é—´: {task_info.get('submitted_at', 'æœªçŸ¥æ—¶é—´')}")
        
        # # æ·»åŠ æŒ‰é’®å¯¼èˆªåˆ°è¯„åˆ†æŠ¥å‘Šå’Œå¯è§†åŒ–é¡µé¢
        # col1, col2, col3 = st.columns(3)
        # with col1:
        #     if st.button("ğŸ  è¿”å›é¦–é¡µ", use_container_width=True):
        #         st.switch_page("main.py")
        
        # with col2:
        #     if st.button("ğŸ“Š æŸ¥çœ‹è¯„åˆ†æŠ¥å‘Š", use_container_width=True):
        #         # Set the selected job ID in session state for the score report page
        #         st.session_state.selected_job_id = selected_job
        #         st.switch_page("pages/score_report.py")
        
        # with col3:
        #     if st.button("ğŸ“ˆ æŸ¥çœ‹å¯è§†åŒ–åˆ†æ", use_container_width=True):
        #         # Set the selected job ID in session state for the visualization page
        #         st.session_state.selected_job_id = selected_job
        #         st.switch_page("pages/visualization.py")
        
        # st.markdown("---")
        
        # è·å–æ‰¹æ”¹ç»“æœ
        try:
            response = requests.get(
                f"{st.session_state.backend}/ai_grading/grade_result/{selected_job}",
                timeout=10
            )
            response.raise_for_status()
            result = response.json()
            
            status = result.get("status", "æœªçŸ¥")
            st.write(f"çŠ¶æ€: {status}")
            st.markdown("---")
            
            # Key fix: Check if we have data even if status is not explicitly "completed"
            has_data = "results" in result or "corrections" in result
            
            if status == "completed" or has_data:
                # æ˜¾ç¤ºç»“æœ
                if "results" in result:  # Batch grading results
                    all_results = result["results"]
                    st.subheader("æ‰€æœ‰å­¦ç”Ÿæ‰¹æ”¹ç»“æœ")
                    
                    all_results.sort(key=lambda s: s['student_id'])
                    for student_result in all_results:
                        student_id = student_result["student_id"]
                        corrections = student_result["corrections"]
                        
                        # corrections.sort(key=lambda c: int(c['q_id'][1:]))
                        corrections.sort(key=lambda c: natural_sort_key(c['q_id']))

                        st.markdown(f"### å­¦ç”Ÿ: {student_id}")
                        
                        # å‡†å¤‡æ•°æ®ç”¨äºæ˜¾ç¤º
                        data = []
                        total_score = 0
                        total_max_score = 0
                        
                        for correction in corrections:
                            # ç›´æ¥ä½¿ç”¨è¿”å›çš„ç±»å‹ï¼Œå¦‚æœå·²ç»æ˜¯ä¸­æ–‡åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™è¿›è¡Œæ˜ å°„
                            question_type = correction["type"]
                            if question_type in type_display_mapping:
                                display_type = type_display_mapping[question_type]
                            elif question_type in type_display_mapping.values():
                                display_type = question_type
                            else:
                                display_type = "æ¦‚å¿µé¢˜"  # é»˜è®¤ç±»å‹
                            
                            data.append({
                                "é¢˜å·": correction["q_id"][1:],
                                "é¢˜ç›®ç±»å‹": display_type,  # ä½¿ç”¨ä¸­æ–‡æ˜¾ç¤ºç±»å‹
                                "å¾—åˆ†": f"{correction['score']:.1f}",
                                "æ»¡åˆ†": f"{correction['max_score']:.1f}",
                                "ç½®ä¿¡åº¦": f"{correction['confidence']:.2f}",
                                "è¯„è¯­": correction["comment"]
                            })
                            total_score += correction["score"]
                            total_max_score += correction["max_score"]
                        
                        # æ˜¾ç¤ºè¯¥å­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœè¡¨æ ¼
                        df = pd.DataFrame(data)
                        st.dataframe(df, use_container_width=True)
                        
                        # æ˜¾ç¤ºæ€»åˆ†
                        st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                        st.divider()
                        
                elif "corrections" in result:  # Single student grading results
                    corrections = result["corrections"]
                    st.subheader(f"å­¦ç”Ÿ {result.get('student_id', 'æœªçŸ¥å­¦ç”Ÿ')} çš„æ‰¹æ”¹ç»“æœ")
                    
                    # å‡†å¤‡æ•°æ®ç”¨äºæ˜¾ç¤º
                    # corrections.sort(key=lambda c: int(c['q_id'][1:]))
                    corrections.sort(key=lambda c: natural_sort_key(c['q_id']))

                    data = []
                    total_score = 0
                    total_max_score = 0
                    
                    for correction in corrections:
                        # ç›´æ¥ä½¿ç”¨è¿”å›çš„ç±»å‹ï¼Œå¦‚æœå·²ç»æ˜¯ä¸­æ–‡åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™è¿›è¡Œæ˜ å°„
                        question_type = correction["type"]
                        if question_type in type_display_mapping:
                            display_type = type_display_mapping[question_type]
                        elif question_type in type_display_mapping.values():
                            display_type = question_type
                        else:
                            display_type = "æ¦‚å¿µé¢˜"  # é»˜è®¤ç±»å‹
                        
                        data.append({
                            "é¢˜å·": correction["q_id"][1:],
                            "é¢˜ç›®ç±»å‹": display_type,  # ä½¿ç”¨ä¸­æ–‡æ˜¾ç¤ºç±»å‹
                            "å¾—åˆ†": f"{correction['score']:.1f}",
                            "æ»¡åˆ†": f"{correction['max_score']:.1f}",
                            "ç½®ä¿¡åº¦": f"{correction['confidence']:.2f}",
                            "è¯„è¯­": correction["comment"]
                        })
                        total_score += correction["score"]
                        total_max_score += correction["max_score"]
                    
                    # æ˜¾ç¤ºæ‰¹æ”¹ç»“æœè¡¨æ ¼
                    df = pd.DataFrame(data)
                    st.dataframe(df, use_container_width=True)
                    
                    # æ˜¾ç¤ºæ€»åˆ†
                    st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                else:
                    st.warning("æ‰¹æ”¹ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°å­¦ç”Ÿæ•°æ®ã€‚")
            elif status == "error":
                st.error(f"æ‰¹æ”¹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            elif status == "pending":
                st.info("æ‰¹æ”¹ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨å€™...")
                
                # Show mock data while waiting
                st.info("æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®ä»¥ä¾›é¢„è§ˆ")
                try:
                    from frontend_utils.data_loader import load_mock_data
                    mock_data = load_mock_data()
                    
                    if "student_scores" in mock_data:
                        all_mock_students = mock_data["student_scores"]
                        
                        # --- ä¿®æ”¹1ï¼šå¯¹æ¨¡æ‹Ÿå­¦ç”Ÿæ•°æ®æŒ‰å­¦å·æ’åº ---
                        all_mock_students.sort(key=lambda s: s.student_id)
                        
                        st.subheader("æ¨¡æ‹Ÿå­¦ç”Ÿæ‰¹æ”¹ç»“æœé¢„è§ˆ")
                        for student in all_mock_students[:5]:
                            st.markdown(f"### å­¦ç”Ÿ: {student.student_name} ({student.student_id})")
                            
                            # Prepare data for display
                            # --- ä¿®æ”¹2ï¼šå¯¹æ¨¡æ‹Ÿé¢˜ç›®æ•°æ®æŒ‰é¢˜å·æ’åº ---
                            # student.questions.sort(key=lambda q: int(q['question_id'][1:]))
                            student.questions.sort(key=lambda q: natural_sort_key(q['question_id'][1:]))

                            data = []
                            total_score = 0
                            total_max_score = 0
                            
                            for question in student.questions:
                                data.append({
                                    "é¢˜å·": question["question_id"][1:],
                                    "é¢˜ç›®ç±»å‹": question["question_type"],
                                    "å¾—åˆ†": f"{question['score']:.1f}",
                                    "æ»¡åˆ†": f"{question['max_score']:.1f}",
                                    "ç½®ä¿¡åº¦": f"{question['confidence']:.2f}",
                                    "è¯„è¯­": question["feedback"]
                                })
                                total_score += question["score"]
                                total_max_score += question["max_score"]
                            
                            # Display the student's grading results table
                            df = pd.DataFrame(data)
                            st.dataframe(df, use_container_width=True)
                            
                            # Display total score
                            st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                            st.divider()
                except Exception as e:
                    st.warning(f"æ— æ³•åŠ è½½æ¨¡æ‹Ÿæ•°æ®: {e}")
            else:
                st.warning(f"æœªçŸ¥çŠ¶æ€: {status}")
                
        except requests.exceptions.RequestException as e:
            st.error(f"è·å–æ‰¹æ”¹ç»“æœå¤±è´¥: {e}")
            
            # Show mock data when backend is not available
            st.info("æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®")
            try:
                from frontend_utils.data_loader import load_mock_data
                mock_data = load_mock_data()
                
                if "student_scores" in mock_data:
                    all_mock_students = mock_data["student_scores"]
                    
                    # --- ä¿®æ”¹3ï¼šåœ¨è¯·æ±‚å¼‚å¸¸çš„æ¨¡æ‹Ÿæ•°æ®ä¸­ï¼ŒåŒæ ·æ·»åŠ æ’åºé€»è¾‘ ---
                    all_mock_students.sort(key=lambda s: s.student_id)
                    
                    st.subheader("æ¨¡æ‹Ÿå­¦ç”Ÿæ‰¹æ”¹ç»“æœ")
                    for student in all_mock_students[:5]:
                        st.markdown(f"### å­¦ç”Ÿ: {student.student_name} ({student.student_id})")
                        
                        # Prepare data for display
                        # student.questions.sort(key=lambda q: int(q['question_id'][1:]))
                        student.questions.sort(key=lambda q: natural_sort_key(q['question_id'][1:]))
                        
                        data = []
                        total_score = 0
                        total_max_score = 0
                        
                        for question in student.questions:
                            data.append({
                                "é¢˜å·": question["question_id"][1:],
                                "é¢˜ç›®ç±»å‹": question["question_type"],
                                "å¾—åˆ†": f"{question['score']:.1f}",
                                "æ»¡åˆ†": f"{question['max_score']:.1f}",
                                "ç½®ä¿¡åº¦": f"{question['confidence']:.2f}",
                                "è¯„è¯­": question["feedback"]
                            })
                            total_score += question["score"]
                            total_max_score += question["max_score"]
                        
                        # Display the student's grading results table
                        df = pd.DataFrame(data)
                        st.dataframe(df, use_container_width=True)
                        
                        # Display total score
                        st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                        st.divider()
            except Exception as e:
                st.warning(f"æ— æ³•åŠ è½½æ¨¡æ‹Ÿæ•°æ®: {e}")
        except Exception as e:
            st.error(f"å¤„ç†æ‰¹æ”¹ç»“æœæ—¶å‡ºç°é”™è¯¯: {e}")
            
            # Show mock data when there's an error
            st.info("æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®")
            try:
                from frontend_utils.data_loader import load_mock_data
                mock_data = load_mock_data()
                
                if "student_scores" in mock_data:
                    all_mock_students = mock_data["student_scores"]
                    
                    # --- ä¿®æ”¹4ï¼šåœ¨å…¶ä»–å¼‚å¸¸çš„æ¨¡æ‹Ÿæ•°æ®ä¸­ï¼Œä¹Ÿæ·»åŠ æ’åºé€»è¾‘ ---
                    all_mock_students.sort(key=lambda s: s.student_id)
                    
                    st.subheader("æ¨¡æ‹Ÿå­¦ç”Ÿæ‰¹æ”¹ç»“æœ")
                    for student in all_mock_students[:5]:
                        st.markdown(f"### å­¦ç”Ÿ: {student.student_name} ({student.student_id})")
                        
                        # Prepare data for display
                        # student.questions.sort(key=lambda q: int(q['question_id'][1:]))
                        student.questions.sort(key=lambda q: natural_sort_key(q['question_id'][1:]))

                        data = []
                        total_score = 0
                        total_max_score = 0
                        
                        for question in student.questions:
                            data.append({
                                "é¢˜å·": question["question_id"][1:],
                                "é¢˜ç›®ç±»å‹": question["question_type"],
                                "å¾—åˆ†": f"{question['score']:.1f}",
                                "æ»¡åˆ†": f"{question['max_score']:.1f}",
                                "ç½®ä¿¡åº¦": f"{question['confidence']:.2f}",
                                "è¯„è¯­": question["feedback"]
                            })
                            total_score += question["score"]
                            total_max_score += question["max_score"]
                        
                        # Display the student's grading results table
                        df = pd.DataFrame(data)
                        st.dataframe(df, use_container_width=True)
                        
                        # Display total score
                        st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                        st.divider()
            except Exception as e:
                st.warning(f"æ— æ³•åŠ è½½æ¨¡æ‹Ÿæ•°æ®: {e}")

inject_pollers_for_active_jobs()

def return_top():
    scroll_to_here(50, key='top')
    scroll_to_here(0, key='top_fix')

# Add a link back to the history page

col1, _, col2 = st.columns([8, 40, 12])

with col1:
    st.button(
        "è¿”å›é¡¶éƒ¨", 
        on_click=return_top,
        use_container_width=False
    )

with col2:
    # 2. åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œå¹¶å‘Šè¯‰å®ƒåœ¨è¢«ç‚¹å‡»æ—¶è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
    st.page_link("pages/history.py", label="è¿”å›å†å²æ‰¹æ”¹è®°å½•", icon="â¡ï¸")